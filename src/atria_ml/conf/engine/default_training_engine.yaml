_target_: atria_ml.training.engines.training.TrainingEngine
_partial_: false
defaults:
- _self_
- /optimizer_factory@optimizer_factory: adam
- /lr_scheduler_factory@lr_scheduler_factory: cosine_annealing_lr
optimizer_factory: ???
max_epochs: 100
epoch_length: null
outputs_to_running_avg: null
logging:
  _target_: atria_ml.training.configs.logging_config.LoggingConfig
  logging_steps: 100
  refresh_rate: 10
  log_gpu_stats: false
  profile_time: false
  log_to_tb: true
metric_logging_prefix: null
sync_batchnorm: false
test_run: false
use_fixed_batch_iterator: false
eval_training: false
stop_on_nan: true
clear_cuda_cache: true
model_ema_config:
  _target_: atria_ml.training.configs.model_ema_config.ModelEmaConfig
  enabled: false
  momentum: 0.0001
  momentum_warmup: 0.0
  warmup_iters: 0
  update_every: 1
warmup_config:
  _target_: atria_ml.training.configs.warmup_config.WarmupConfig
  warmup_ratio: 0
  warmup_steps: 0
early_stopping:
  _target_: atria_ml.training.configs.early_stopping_config.EarlyStoppingConfig
  enabled: false
  monitored_metric: val/loss
  min_delta: 0.0
  patience: 3
  cumulative_delta: false
  mode: min
model_checkpoint_config:
  _target_: atria_ml.training.configs.model_checkpoint.ModelCheckpointConfig
  enabled: true
  dir: checkpoints
  n_saved: 1
  n_best_saved: 1
  monitored_metric: validation/running_avg_loss
  mode: min
  name_prefix: ''
  save_weights_only: false
  load_weights_only: false
  every_n_steps: null
  every_n_epochs: 1
  load_best_checkpoint_resume: false
  resume_from_checkpoint: true
  resume_checkpoint_file: null
gradient_config:
  _target_: atria_ml.training.configs.gradient_config.GradientConfig
  enable_grad_clipping: false
  max_grad_norm: 1.0
  gradient_accumulation_steps: 1
with_amp: false
